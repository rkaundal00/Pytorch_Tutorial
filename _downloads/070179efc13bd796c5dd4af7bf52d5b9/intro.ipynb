{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aiulh-YnkvGu"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A667BjwkvGz"
      },
      "source": [
        "**Learn the Basics** \\|\\| [Quickstart](quickstart_tutorial.html) \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
        "Model](buildmodel_tutorial.html) \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Learn the Basics\n",
        "================\n",
        "\n",
        "Authors: [Suraj Subramanian](https://github.com/subramen), [Seth\n",
        "Juarez](https://github.com/sethjuarez/), [Cassie\n",
        "Breviu](https://github.com/cassiebreviu/), [Dmitry\n",
        "Soshnikov](https://soshnikov.com/), [Ari\n",
        "Bornstein](https://github.com/aribornstein/)\n",
        "\n",
        "Most machine learning workflows involve working with data, creating\n",
        "models, optimizing model parameters, and saving the trained models. This\n",
        "tutorial introduces you to a complete ML workflow implemented in\n",
        "PyTorch, with links to learn more about each of these concepts.\n",
        "\n",
        "We\\'ll use the FashionMNIST dataset to train a neural network that\n",
        "predicts if an input image belongs to one of the following classes:\n",
        "T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,\n",
        "Bag, or Ankle boot.\n",
        "\n",
        "[This tutorial assumes a basic familiarity with Python and Deep Learning\n",
        "concepts.]{.title-ref}\n",
        "\n",
        "Running the Tutorial Code\n",
        "-------------------------\n",
        "\n",
        "You can run this tutorial in a couple of ways:\n",
        "\n",
        "-   **In the cloud**: This is the easiest way to get started! Each\n",
        "    section has a \\\"Run in Microsoft Learn\\\" and \\\"Run in Google Colab\\\"\n",
        "    link at the top, which opens an integrated notebook in Microsoft\n",
        "    Learn or Google Colab, respectively, with the code in a fully-hosted\n",
        "    environment.\n",
        "-   **Locally**: This option requires you to setup PyTorch and\n",
        "    TorchVision first on your local machine ([installation\n",
        "    instructions](https://pytorch.org/get-started/locally/)). Download\n",
        "    the notebook or copy the code into your favorite IDE.\n",
        "\n",
        "How to Use this Guide\n",
        "---------------------\n",
        "\n",
        "If you\\'re familiar with other deep learning frameworks, check out the\n",
        "[0. Quickstart](quickstart_tutorial.html) first to quickly familiarize\n",
        "yourself with PyTorch\\'s API.\n",
        "\n",
        "If you\\'re new to deep learning frameworks, head right into the first\n",
        "section of our step-by-step guide: [1. Tensors](tensor_tutorial.html).\n",
        "\n",
        "::: {.toctree maxdepth=\"2\" hidden=\"\"}\n",
        "quickstart\\_tutorial tensorqs\\_tutorial data\\_tutorial\n",
        "transforms\\_tutorial buildmodel\\_tutorial autogradqs\\_tutorial\n",
        "optimization\\_tutorial saveloadrun\\_tutorial\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors"
      ],
      "metadata": {
        "id": "8vCq0bszSA0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mcpzzEbnlDku"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "n2oXh3O9sSVj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "hmvnV9TVsjcI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The new tensor retains the properties (shape, datatype) of the argument tensor,\n",
        "# unless explicitly overridden\n",
        "x_ones = torch.ones_like(x_data)\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "id": "IJtaWudEswLh",
        "outputId": "3c138e51-1b01-461f-926a-853c3220c887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.4824, 0.1241],\n",
            "        [0.6773, 0.6260]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "id": "bWw3jgIntpAy",
        "outputId": "af356eda-ee94-4abc-e174-6d672c53bbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.3265, 0.4394, 0.2613],\n",
            "        [0.5371, 0.7667, 0.9718]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "id": "dqum4Vy-uDCN",
        "outputId": "56712890-3656-4bca-c003-47c86df17a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to(\"cuda\")"
      ],
      "metadata": {
        "id": "al1O4fT_u6Pf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\") # What does ... mean?\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "1NGkqpQtvJ2s",
        "outputId": "81420ee5-494c-4af8-8168-6b03fd0f9606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ],
      "metadata": {
        "id": "QBn-yYiQvkCQ",
        "outputId": "126bb5a3-8d4e-4f17-a996-f379d02b2f2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxM_L9dxDcMr",
        "outputId": "cb4c5d90-9174-4dbd-808a-17b1733abd59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single-element tensors** If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item():`"
      ],
      "metadata": {
        "id": "jSjWcdFsEX9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7WF41y3D4to",
        "outputId": "c01402a9-cfdc-4460-aebc-2b9d2091b134"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In-place operations** Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: `x.copy_(y)`, `x.t_()`, will change x."
      ],
      "metadata": {
        "id": "o4bYCqu5E-4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e68-nw84FKop",
        "outputId": "3abfa45d-4b2e-4d3c-e80a-fc05a2a74317"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE5v8rTtGGbX",
        "outputId": "7dffe88b-73b8-47af-e376-1e904ca42e6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP7U5swQGgZL",
        "outputId": "351319ec-3641-4477-9eea-a8c17bcad252"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ],
      "metadata": {
        "id": "Z0lWMzo3Go6L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZHvtZqUGryV",
        "outputId": "cbca9954-020b-4926-d6ce-a7d50b9ca502"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets & DataLoaders"
      ],
      "metadata": {
        "id": "hHC3z4LqSHwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dataset* stores the samples and their corresponding labels, and DataLoader wraps an iterable around the *Dataset* to enable easy access to the samples."
      ],
      "metadata": {
        "id": "xsxt60pGSaHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WTLmsR0SQB_",
        "outputId": "38f4ae60-d4b6-4833-9176-86d5c4a5eae0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 9.94MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 209kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.90MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 6.30MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "TM2_mwCgTyhi",
        "outputId": "a95d5c11-f50a-4a21-f12f-c992e4b3c933"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKSCAYAAABMVtaZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABoZElEQVR4nO3deXgUZbb48ROy7wlJIBAgYZFdRVFgVIZVGFBRBxcQWVwZ9/U66jijjHfG64gOboB676AiI4qKK4io4IKDuKICAiKrkoQl+77U7w8f8jPkPS900SGB9/t5Hp8ZTvXpqu6u6joUfU6FeJ7nCQAAAI56LZp6AwAAAHB4UPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgdBiEhIXLPPffU/fnpp5+WkJAQ2bJlS5NtE3A04lgD/JsyZYrExcUd8HGDBw+WwYMHN/4GoVFQ+BnsO1ns+y8qKkq6du0q1157reTk5DT15gFHDY414NDMnDlTQkJCpH///k29Kb5NmTKl3vdAWFiYtG/fXsaNGydr165t1HWXlpbKPffcI8uXL2/U9TQnYU29Ac3ZX//6V+nYsaOUl5fLxx9/LLNmzZJFixbJd999JzExMU29ecBRg2MN8GfevHmSlZUlq1atkh9++EG6dOnS1JvkS2RkpPzv//6viIhUV1fLpk2bZPbs2fL222/L2rVrpW3bto2y3tLSUpk2bZqIiDNXMSn8LEaNGiUnnXSSiIhcfvnlkpKSIg899JC89tprMn78+CbeusZTUlIisbGxTb0ZcAjHGhC4zZs3yyeffCKvvPKKTJ06VebNmyd33313U2+WL2FhYXLxxRfXiw0YMEDOPPNMeeutt+SKK65ooi07+vBPvQEYOnSoiPxysGm/cZgyZYpkZWX5ev6ZM2dKr169JDIyUtq2bSvXXHON5Ofn1y2/9tprJS4uTkpLSxvkjh8/XtLT06WmpqYutnjxYhk4cKDExsZKfHy8nHHGGbJmzZoG2xsXFyebNm2S0aNHS3x8vEyYMMHX9gPBwrEGHNi8efMkOTlZzjjjDDnvvPNk3rx5DR6zZcsWCQkJkenTp8uTTz4pnTt3lsjISDn55JPls88+O+A6vv76a0lLS5PBgwdLcXGx+riKigq5++67pUuXLhIZGSnt27eX2267TSoqKny/vvT0dBH5pSj8tR9//FHOP/98admypcTExMiAAQPkrbfeapCfm5srl112mbRu3VqioqLk+OOPl2eeeaZu+ZYtWyQtLU1ERKZNm1b3T82//p3w0YjCLwCbNm0SEZGUlJSgP/c999wj11xzjbRt21YefPBBGTt2rDzxxBMyYsQIqaqqEhGRCy+8UEpKShrs4KWlpfLGG2/IeeedJ6GhoSIiMnfuXDnjjDMkLi5O7r//fvnzn/8sa9euldNOO63BD92rq6tl5MiR0qpVK5k+fbqMHTs26K8PCATHGnBg8+bNk9///vcSEREh48ePl40bN6rF3L///W954IEHZOrUqfLf//3fsmXLFvn9739ft8+bfPbZZzJ06FA54YQTZPHixWrjR21trYwZM0amT58uZ511ljz66KNyzjnnyD//+U+58MILD/r17N69W3bv3i05OTnyn//8R2666SZJSUmRM888s+4xOTk5csopp8iSJUvk6quvlr/97W9SXl4uY8aMkYULF9Y9rqysTAYPHixz586VCRMmyAMPPCCJiYkyZcoUefjhh0VEJC0tTWbNmiUiIueee67MnTtX5s6dK7///e8PepuPSB4amDNnjici3rvvvuvt2rXL2759uzd//nwvJSXFi46O9nbs2OENGjTIGzRoUIPcyZMne5mZmfViIuLdfffdDZ5/8+bNnud5Xm5urhcREeGNGDHCq6mpqXvcY4895omI969//cvzPM+rra31MjIyvLFjx9Z7/hdffNETEe/DDz/0PM/zioqKvKSkJO+KK66o97js7GwvMTGxXnzy5MmeiHi33357oG8TcMg41gB/Pv/8c09EvKVLl3qe98s+265dO++GG26o97jNmzd7IuKlpKR4e/furYu/9tprnoh4b7zxRl1s8uTJXmxsrOd5nvfxxx97CQkJ3hlnnOGVl5fXe879j8m5c+d6LVq08D766KN6j5s9e7YnIt6KFSusr2XfsbH/fxkZGd4XX3xR77E33nijJyL11lVUVOR17NjRy8rKqjuuZ8yY4YmI99xzz9U9rrKy0vvNb37jxcXFeYWFhZ7ned6uXbsafG8c7bjiZzF8+HBJS0ur6y6Ki4uThQsXSkZGRlDX8+6770plZaXceOON0qLF//9IrrjiCklISKi76hASEiLnn3++LFq0qN4l9xdeeEEyMjLktNNOExGRpUuXSn5+vowfP77ub1C7d++W0NBQ6d+/vyxbtqzBNlx11VVBfU1AIDjWgMDMmzdPWrduLUOGDBGRX/bZCy+8UObPn1/vZwj7XHjhhZKcnFz354EDB4rIL/9sur9ly5bJyJEjZdiwYfLKK69IZGSkdVsWLFggPXr0kO7du9c7Dvb9ZMN0HOwvKipKli5dKkuXLpUlS5bIE088IXFxcTJ69GjZsGFD3eMWLVok/fr1qzsGRUTi4uLkyiuvlC1bttR1AS9atEjS09Pr/UY4PDxcrr/+eikuLpYPPvjggNt0tKK5w+Lxxx+Xrl27SlhYmLRu3Vq6detW72QRLFu3bhURkW7dutWLR0RESKdOneqWi/xy8M6YMUNef/11ueiii6S4uFgWLVokU6dOlZCQEBER2bhxo4j8/99J7S8hIaHen8PCwqRdu3ZBez1AoDjWgINXU1Mj8+fPlyFDhsjmzZvr4v3795cHH3xQ3nvvPRkxYkS9nA4dOtT7874iMC8vr168vLxczjjjDOnbt6+8+OKLDX5fZ7Jx40ZZt25d3e/l9pebm3vA5wgNDZXhw4fXi40ePVqOOeYYueOOO+Tll18WkV+OYdPomh49etQt7927t2zdulWOOeaYBt8jv36cqyj8LPr161fXabi/kJAQ8TyvQdz0N61gGjBggGRlZcmLL74oF110kbzxxhtSVlZW73cUtbW1IvLLb4/2/Tj21/Y/kCMjIxvlJAscLI414OC9//77snPnTpk/f77Mnz+/wfJ58+Y1KPz2/SZ1f/sfW5GRkTJ69Gh57bXX5O233673+zpNbW2tHHvssfLQQw8Zl7dv3/6Az2HSrl076datm3z44Ye+8mFG4edTcnKy8RK5n79FZGZmiojI+vXrpVOnTnXxyspK2bx5c4O/BV1wwQXy8MMPS2FhobzwwguSlZUlAwYMqFveuXNnERFp1apVg1zgSMOxBtQ3b948adWqlTz++OMNlr3yyiuycOFCmT17tkRHRwf83CEhITJv3jw5++yz5fzzz5fFixcfcL5d586dZfXq1TJs2LC6q+HBUl1dXe/nFpmZmbJ+/foGj/v+++/rlu/732+++UZqa2vr/WVr/8cFe3uPBPzV06fOnTvL999/L7t27aqLrV69WlasWBHwcw0fPlwiIiLkkUceqfe3r//7v/+TgoICOeOMM+o9/sILL5SKigp55pln5O2335YLLrig3vKRI0dKQkKC/P3vfzd2bP16m4HmjmMN+P/KysrklVdekTPPPFPOO++8Bv9de+21UlRUJK+//rrvdURERMgrr7wiJ598spx11lmyatUq6+MvuOAC+emnn+Spp54ybm9JSYmv7diwYYOsX79ejj/++LrY6NGjZdWqVfKf//ynLlZSUiJPPvmkZGVlSc+ePesel52dLS+88ELd46qrq+XRRx+VuLg4GTRokIhI3YD4X49zOtpxxc+nSy+9VB566CEZOXKkXHbZZZKbmyuzZ8+WXr16SWFhYUDPlZaWJnfccYdMmzZNfve738mYMWNk/fr1MnPmTDn55JMbDLU88cQTpUuXLvKnP/1JKioqGrTLJyQkyKxZs2TixIly4oknyrhx4yQtLU22bdsmb731lpx66qny2GOPHfJ7ABwOHGvA//f6669LUVGRjBkzxrh8wIABkpaWJvPmzQtolMr+oqOj5c0335ShQ4fKqFGj5IMPPpDevXsbHztx4kR58cUX5Q9/+IMsW7ZMTj31VKmpqZHvv/9eXnzxRVmyZIn6U459qqur5bnnnhORX/7peMuWLTJ79mypra2tN5T69ttvl+eff15GjRol119/vbRs2VKeeeYZ2bx5s7z88st1V/euvPJKeeKJJ2TKlCnyxRdfSFZWlrz00kuyYsUKmTFjhsTHx9e9zp49e8oLL7wgXbt2lZYtW0rv3r3V13pUaNKe4mZq3wiIzz77zPq45557zuvUqZMXERHh9enTx1uyZImvERP7PPbYY1737t298PBwr3Xr1t5VV13l5eXlGdf9pz/9yRMRr0uXLur2LVu2zBs5cqSXmJjoRUVFeZ07d/amTJniff7553WP+XX7PnC4cawBgTnrrLO8qKgor6SkRH3MlClTvPDwcG/37t1141weeOCBBo/b/3gx7aO7d+/2evbs6aWnp3sbN270PK/hOBfP+2VUyv333+/16tXLi4yM9JKTk72+fft606ZN8woKCqyvyTTOJSEhwRs2bJj37rvvNnj8pk2bvPPOO89LSkryoqKivH79+nlvvvlmg8fl5OR4l1xyiZeamupFRER4xx57rDdnzpwGj/vkk0+8vn37ehEREU6MdgnxPMOvpgEAAHDU4Td+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA44qDv3NGc72e37557+0tLS1NztBul23JatWpljCcmJqo5+27ivj/T7Z0OxJajjWO03SqnoqLCGLfdRH7v3r3G+Jo1a9Sc3NxcdVlTa45jLJvzsYamZ9s/muP+vE9z3DaONRyNDnSsccUPAADAERR+AAAAjqDwAwAAcASFHwAAgCNCvIP8xW0wfwQb7B8nP/zww8Z4QkKCmtOhQwdj/Oeff1ZztG1LSkpSc7SGjNTU1IDXY3vfCgsLjfH8/Hw1x7bdmk2bNhnjeXl5as60adMCXs/hwg/Om8748ePVZeecc44x/tFHH6k5X331lTEeFqb3sMXHxxvjW7ZsUXO04+byyy9Xc9avX2+M33fffWqOH1pjltZkdjhxrDUdP6+zOXxeGRkZxnifPn3UnGOPPdYY9/MeVFdXq8u075UffvhBzdG+o2w5ftDcAQAAABGh8AMAAHAGhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAARxz0vXqDKdjjXLZv326Mt27dOuDn0sa82Nazc+dONaeystIYj4uLU3N2795tjNvG02jrsb2eH3/80Rjftm2bmqPdyzg6OlrNwdHvsssuU5fdeeedxrh272sRfSzJBRdcENiGiT5KRURk1apVxviECRPUHG3bysrK1BxtLMTVV1+t5ixfvtwYnzhxopqjjW0JDQ1Vc2pqatRlODrY7r/u5/M/7rjjjPFzzz1XzbnwwguN8fbt26s5tvPkkUirFQoKCtQc7Xw8cuRI39vBFT8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcESId5BttM35ZtbLli0zxtetW6fmFBcXG+O2t6Nnz57GuNYZKCISHh5ujFdUVKg5WmeerTMrPz/fGO/evbuak52dbYyXlJSoOaeffroxnpKSouYMGDDAGA92d7cfzeFG5Ps7XMeath7be6J183366adqTmFhoTFeWlqq5mjHgG3bYmNjjfGcnBw1R+vU116niN6ha9s27XvA9llrHfRr1qxRc0466SR1mUb7XtE+A79cPtaaWkxMjLpMOw5HjRql5rz88svGeFVVlZqjfQ/k5eWpOWvXrlWXabQJE6mpqWqOtt22fTYszDwUJSkpSc3Rjqn4+Hg1RztPn3LKKWrOgY41rvgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxh7kc+wrzwwgvG+IgRI9Scr776yhjXRkKI6DdYto1+eP/99wNej9Z6bxuvoI1tKSoqUnO0dnTbtmnPd/fdd6s5muY43sElft7/J554whi3jXHQxgNp+5+ISEREhDFeWVmp5mgjmlq3bq3mdO3a1RjXRijYREZGBpxj89NPPxnj2lgpEZEbbrjBGH/44YeDsk04MmkjiGx+/vlnddnWrVuNce0YFNGP99DQUDXnt7/9bcDr2bNnjzGenp6u5mjbYNu2mpqagOIiImVlZca47dy+adMmdZlfXPEDAABwBIUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEccMV29tptpf/3118Z4v379An4+7YblIiKbN282xm3dfNqNlG03WtfYOoyysrKM8Y8//ljNadu2rTGudR6JiGzYsMEYHzJkiJpzzDHHGOPr1q1Tc7TPFE2rZcuWxrit21Y7Pmzdb9oyW/ebdnzYOgC1LvXw8HA1R3s9tu8orevZ9n2j3Wy+vLxczdEmGdi6em3vKY4Ofjr4beeBhIQEY9w2ESI5OdkYtx2fO3bsMMZt59x27doZ47bJA7bXqtG6lLWJBCL652D77ujSpUtgG3YQuOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHBEiHeQfd62duNA9e7dW13217/+1RjPyclRc7Q27V27dqk5Wst1fHy8mqONPbCNpbj11luN8XHjxqk5Wtt7q1at1BxtzIbtPejVq5cx/u2336o5mri4OHWZtu/Ycu69915j3PZ6/PAz5qCxBfNY86N79+7qss8++8wYLykpUXO0MSu2sSTasWb7vGzHYaDrsY1Z0V6PnxwbbaSMbT3ad2Hnzp3VnNLS0sA2zCeOtaZj22e0Y0Ab2SIisnjx4oCeS0Tkhx9+MMa3bt2q5vTv398Y79Gjh5qj7We290D77rCNedHGudjeA210TVRUlJqjfQ6ZmZlqzoGONa74AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjzG0pjWzDhg3qsi1bthjj+fn5ao6fblstx3aDZY1t2x599FFjfMiQIWqOn5s/r1u3zhi3dc6uXbvWGLd1JWndR7YbbcfExBjje/fuVXOC3b2Lgzd27Fh1mbYPVlRUqDlaR2uwuzxtXXvBzNG+V/y8HltXqXas2bqhtYkAY8aMUXPmz5+vLsPRwdZVrn3fFxYWqjknnHCCMb5582Y1Z/jw4cZ4SkqKmlNZWWmM28651dXVxrjtPYiOjjbGtXOxiP49YMvRjmlt/SIin376qbrML674AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAc0STjXLQWbRF9LIh2w3IRfbyBbVSDNkbB1iauPZ+tfXvnzp3GeMuWLdUcbWTGxo0b1RxNUVGRuszP2BhtZIVtBIz2fLb9AE3nzDPPVJdpn5ntGNCOG9s+o+XYjmltmW2sk22cSqBs69GW2cY4aO+pbSyFdnyeddZZag7jXI5+tmPND+28YjuetHNrSUmJmhMZGWmMl5aWqjnamDDbd4dWX9iONe21atssoo+asX0P2Eay+cUVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwRJN09dokJCQY47YuHq1TxtZdo3U5+elO9HNz9qqqKnWZ1klk2zatw8hP16Jt22zboNG24ccffwz4udD4+vfvry7bs2ePMR4bG6vmaB2tto45TbBzgtntaFuPny5l7Ybutm547dgdMGCAmoOjX7C7enfv3m2M26ZvaOdj7Zwvop9btXO+iEhqaqoxnpiYqOZox5Tt3G7rxNVox6ftfXviiScCXs+BcMUPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOCIZjfOJScnxxhPTk4O+Ln8jFmx0VribSNTtPEnZWVlao62zNbCrq0nIiJCzdHGRdjWo7Wj225Qr30OhYWFag4an7ZvaDc5F9E/Z9tYEu34sI0/0dYT7GNa224/+7PtPQjmOA3b6Adt7FWnTp2Ctn4ceYJ93Hz00UfG+MCBA9WcHTt2GOO2UVDaec02Vkw7r9nONyUlJca47TuqoKDAGLeNkdM+B9t61q9fry7ziyt+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOCIZtfVW15ebozbOtmCfQNqjZ/OKC3H1gmssXUNap1ERUVFas4PP/xgjMfExKg52nbHx8erOVrH0uH63GCWnp5ujNuONe1m5ja2DjyNbV8PpmDug7bOPO19C/brDHb3Jo5+Wne/7Vj/+OOPjfGxY8eqOdp5xfb94Oe7Q6shbLTzWkVFhZqjvW+2c6HWPWybIqB15K9evVrNORCu+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHNHsxrloIxH83DQ92ONC/Ixg8ZOjsY1+0N63/Px8NUdrybeNcznhhBOM8VatWqk52meXkpKi5qDpJCQkqMtKS0uNcW20gYh+DNhGNWhjFGzHgLYePyNObMdtMMc62UbAaNtgew/8jIdJSkoyxm3fHXCbtm/GxsaqOdXV1cZ4XFycmqMdH36+B2zjabTn07ZZRD+mbevRXo/tXPjGG2+oy/ziih8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOKLZdfVqHT62ThmtO9DWMadpzjc5t3UylZWVGeMdO3ZUc/r162eM2zoau3TpYozbPh9tWUZGhpqDxmfrxNb46br//vvvjfF27dqpOVVVVca4n65V27b5EczuYT+TB2zd0H62QbupPF29btCONZusrCxjPC8vT83ROvVt5xtt22zHjXYMhIeHqzna80VGRqo5WsevbT2BPpeI3ildUFAQ8Hr24YofAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARzW6cS3p6ujGem5ur5mg3iLeNftBGPByucS62FvZgKi8vV5d1797dGLe1ymvt9baRANr4idTUVDUHjS8tLS3gHG2ci22MQ3Z2tjFuG+eiHZ+2EU3afms7poN5HNq2Tfsush2f2nbbttnPuBttRNP27dsDfi40T7Z9xs85z8+5QxvnYhu3pI05se3n2vkmKipKzSkuLlaXaQoLC43x6OhoNUc73ouKitScO+64wxi//fbbLVtnxxU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHBEs+vqTUhIMMZ37Nih5mgdPrZuJW2ZrcNI6xbysx5bl5Wfm8BrOZWVlWqO1g3t56b2to4p7flsOdp+oHVSIXBJSUlBey5bh7DWAVhWVqbmaPumH4erq9fW0ai9Hls3vHbs2t4bP69Hm6SAo0ewp1Vo3882Woeu7bjRzu22c5R2fERGRqo5Wret7XiKjY01xm2vR3sPbN39kyZNMsbp6gUAAMABUfgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCOaZJzLsGHD1GVaK7bWBu03xw/t+Ww3Z/czXkHLsbWJazm29WvjIvxss+2m2X7Gw/zmN78xxpcsWRLwc8HMz8gUP2MhwsPDjXHtpu0iIjExMca47RjwMzopmII9oqm0tNQYD+aoG5HgjvVB09L2s+joaDVH28+08WUiIj179jTGbeeBuLi4gOIi+r5eUFCg5ti2W6N9R9nO7do4Ktv3mrYeW63Spk0bdZlfXPEDAABwBIUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEc0SVdv165d1WVaV5LtJsZa54+tI0frDtS6bmw5tvVoXU627iet08+Wo22brQNQW4+f7mEbrWPJdoN6rasTwWO7ablG+/xtn6XW/Wbbl7R90NYF6yfncHX8attmO6aDPZVA42c/OFr46bY+XPxMatBonbu255s3b17A69m9e7e6LDEx0RjPy8tTc7TuXds5Nzk52Ri3nQtTUlLUZRrtfYuKilJztO9JW31je3/84oofAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARTTLOZd26deqyAQMGGOOxsbFqjnZTZj83WG7durWao7WW20ZZaG3afkYo2EYM+BkBo7W929rHtRZ2PyNgbOMqtm7dqi5DcPgZmaPtT7YxSNrxads3NX5GWdiOG20bbPuzxrZt2jbYvqO04yPY42lqamoCzjla+Plc/Iz18jOaR9s2P6NmTjzxRHXZtGnTAl7P5s2bjfFjjjlGzdm7d68xXlxcrObExcUFFLfJz89Xl1VUVBjjttEs2ngY22gW7VizfRdr5+n+/furOQfCFT8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcESTdPVq3T0ieteLrWtQy7F1uGmduNHR0WrOrl27jHFbx5bWNRjsG7D76ZzVutBs3VyVlZXGuK1LWVuPrWPKtgzBkZSUFHCO9ln+/PPPao52vGvdaiL+jg8/nbh+Omc1tu5+7fn8dAKXlpaqOX66etGQn07nYH+na7p3764uu/TSS43x3r17qznaFIfU1FQ1Rzsfb9u2Tc3RJnPYJmlo5xXtPCRiPw4DZTvWcnJyjHGtQ1hEn3DQsmVLNUfrYD7hhBPUnAPhih8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBFNMs5Fa2kW0cdF2Fq0tZbvkpISNUcbZVFYWKjmaOMibDeb9zMWQmMb1aBtmy1Ha1W3tZZrYztsNyjXPjvbyAQ/ozkQGG2cSllZmZqj3Zh848aNak6HDh2M8YyMDDVH2zdtx5ptWaBsx612TPkZzeJn/Eowx1WIiLRv3z6oz3c0sI31uu6664zxLl26qDnaqBfbuVAbf2L73tT2M20UmYhIp06djHHbuCdtG2wjbbQRMLb9WTsP2N437T3Qvrtsy8rLy9Uc7fvTz2iz4uJiNUcbXXMotQVX/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAEU3S1WujdarYuka1DiNb509aWpoxrt14WUQkKirKGLd1Avu5yXQwuxNttK7nlJQUNUf7HGw3s7Z1OWmC2Q0NM+0YsHXZacdAfHy8mvP3v//dGB82bJiao3W5+enms3XOasv8dPX66e7XOh1F9A562/vWrVs3Y9zWNdiqVSt1masWL16sLtu+fbsx/s0336g5Wodsfn6+mvPdd98Z46mpqWrO6aefboz37t1bzdH2QdtkhZiYGGPcdp72Q9sG23eU1qFrm1agHbu249NPR76fWkVbVlFREfD69+GKHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAEUfMOBcbreW7devWak5eXp4xbmvR1saf2HK09m1bm7j2HvgZMWG7abbWem+7oXeHDh2M8Z07d6o5thtdo+mkp6cb47YxDtp+ZjsG/vd//zegOOz+8Y9/qMuOP/54Y3zv3r1qjm1809FOG39ie0/i4uKMcdv5RhsT1rNnTzVn6NChAa1fRP9Ot507tFEmtjFc2sgU20gjbfyI7fygjW3x8x1ly9HeU9u2aedw23ugvR7beDctJzExUc05EK74AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjmqSr19b1YrtZsUa7ybjWuSuid/7Yumu0HFv3k9ZJZOsw0jqwbO+bxs9Ns22dwEVFRcZ4mzZt1Jxt27YFvB4/N8BGYJKTk41xrRPdxnazeY1t39SODz/HgI2fKQLavunnuWzfd9p3ka3TUNs222eqdXe7YOzYsca47XPRvrc6deqk5nTu3NkYt30u0dHRxrjtHKXtG1FRUWqOdkzZ9mft+CwrK1NziouLjXGtQ1hEfw9snc3a/mybVvHTTz8Z47Zzu/Z6bOd2jdbxbFuPLedAuOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHBEk4xzsY3x0Nrb/Yw/Cfa4ED+jErQcP2Mp/GyzrSVf225bjnbDaFuOn5EyaHza6AfbsaZ9ll9//XXA6/czNsZPTnPmZ/RDTk5OULfBNh7maHfLLbcY42+//baao40P83Nes303lpSUGOO2UTPa2JZgnzu0USK247Nly5bGeExMjJqjjWbRxoqJiDz22GPG+IknnqjmaJ9paWmpmhPMsWu2kTYaxrkAAADggCj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADii2XX1ajegtnUyad1Ptg4jrfPGT/eTrbtGW4+t88dP56Kf7kCtA8xPV692M23bMlsnk5/Xg8D46VLX9tuCgoKA1287pm3fEUcTP983e/fuDeo2aN+5LtDOHQMHDlRzBgwYYIzfcMMNak67du2M8dTUVDVH63a1HZ9+vje1rm7b93NsbKwxHhkZqeZoy15//XU15/HHHzfGV61apeZonnjiCXVZmzZtjHHb95D2/tjOn36Od62DWetEPhhc8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOKJJxrloY0RERFq3bm2M226WrLVV29ajtcTbRkxobe9t27ZVc7S298LCQjUnPDzcGLe1iWtjAbKzs9Wc4uJiY9w2mkUbf2AbS6DlaDfgFhGJj49XlyE4Nm/ebIz36NFDzdH2wS+++CLg9fsZbXC0sR3Tmk2bNqnLtNFS2mgQEZEvv/wy4G1w2cqVKwOK22RlZanLRo0aZYwPHjxYzenbt68xbvuu1UbAlJWVqTm7du0yxqdOnarmvP322+qyw+E3v/mNuqxly5bGuG08jvae2s6f2nfet99+q+Z069bNGM/Ly1NzDoQrfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgiBDvINvKgtmBZ7u58EUXXWSMb926Vc1JSkoyxm03WNZedmhoqJqjdfXaOlq1Dl2t+05E70bWbtYsIpKcnGyM79ixQ83RXo+fbmjtddpybO/bm2++aYxv2bJFzfHDT1dlYztc3a5a1+D//d//qTlaJ3ivXr3UnKqqKmPc9jqb4+fSGGxTBLTvL1uO1h2oTUsQEbn00kuN8VdffVXN8aM5fqbaPujnGLTl2LpDDwdt8oWIPkFh7969as7h+ixbtDBfm/Lzfp5zzjnqsoyMDGN8/fr1as66deuMcVvdoZ33tXPxgZZpDvT5cMUPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOCIgx7nAgAAgCMbV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgCgePrppyUkJES2bNkScO6UKVMkKysr6NsEHEn2HUOff/75AR87ePBgGTx4cONvlOMo/A7Bvh16339RUVHStm1bGTlypDzyyCNSVFTU1JsIHHG+/fZbOe+88yQzM1OioqIkIyNDTj/9dHn00UebetOAo8avz122/5YvX27Mr62tlWeffVb69+8vLVu2lPj4eOnatatMmjRJVq5c2ejbv3btWrnnnnt8/aXMdWFNvQFHg7/+9a/SsWNHqaqqkuzsbFm+fLnceOON8tBDD8nrr78uxx13XFNvInBE+OSTT2TIkCHSoUMHueKKKyQ9PV22b98uK1eulIcffliuu+66pt5E4Kgwd+7cen9+9tlnZenSpQ3iPXr0MOZff/318vjjj8vZZ58tEyZMkLCwMFm/fr0sXrxYOnXqJAMGDAh4m955552DfuzatWtl2rRpMnjwYK6sB4jCLwhGjRolJ510Ut2f77jjDnn//fflzDPPlDFjxsi6deskOjramFtSUiKxsbGHa1OBZu1vf/ubJCYmymeffSZJSUn1luXm5jbNRgFHoYsvvrjen1euXClLly5tEDfJycmRmTNnyhVXXCFPPvlkvWUzZsyQXbt2+dqmiIiIAz6mvLz8oB4HHf/U20iGDh0qf/7zn2Xr1q3y3HPPicgvv/mJi4uTTZs2yejRoyU+Pl4mTJggIr9cNp8xY4b06tVLoqKipHXr1jJ16lTJy8ur97yff/65jBw5UlJTUyU6Olo6duwol156ab3HzJ8/X/r27Svx8fGSkJAgxx57rDz88MOH54UDh2DTpk3Sq1evBkWfiEirVq3q/v+cOXNk6NCh0qpVK4mMjJSePXvKrFmzGuRkZWXJmWeeKR9//LH069dPoqKipFOnTvLss882eOyaNWtk6NChEh0dLe3atZP//u//ltra2gaPe+211+SMM86Qtm3bSmRkpHTu3FnuvfdeqampObQXDxwhNm/eLJ7nyamnntpgWUhISL1jdZ+Kigq5+eabJS0tTWJjY+Xcc89tUCDu/xu/5cuXS0hIiMyfP1/uuusuycjIkJiYGHnkkUfk/PPPFxGRIUOGHPCfpVEfV/wa0cSJE+XOO++Ud955R6644goREamurpaRI0fKaaedJtOnT5eYmBgREZk6dao8/fTTcskll8j1118vmzdvlscee0y++uorWbFihYSHh0tubq6MGDFC0tLS5Pbbb5ekpCTZsmWLvPLKK3XrXLp0qYwfP16GDRsm999/v4iIrFu3TlasWCE33HDD4X8TgABkZmbKf/7zH/nuu++kd+/e6uNmzZolvXr1kjFjxkhYWJi88cYbcvXVV0ttba1cc8019R77ww8/yHnnnSeXXXaZTJ48Wf71r3/JlClTpG/fvtKrVy8REcnOzpYhQ4ZIdXW13H777RIbGytPPvmk8Ur9008/LXFxcXLzzTdLXFycvP/++/KXv/xFCgsL5YEHHgjuGwI0Q5mZmSIismDBAjn//PPrzmM21113nSQnJ8vdd98tW7ZskRkzZsi1114rL7zwwgFz7733XomIiJBbb71VKioqZMSIEXL99dfLI488InfeeWfdP0dr/yyN/Xjwbc6cOZ6IeJ999pn6mMTERO+EE07wPM/zJk+e7ImId/vtt9d7zEcffeSJiDdv3rx68bfffrtefOHChQdc3w033OAlJCR41dXVfl8W0GTeeecdLzQ01AsNDfV+85vfeLfddpu3ZMkSr7Kyst7jSktLG+SOHDnS69SpU71YZmamJyLehx9+WBfLzc31IiMjvVtuuaUuduONN3oi4n366af1HpeYmOiJiLd582bruqdOnerFxMR45eXldbHJkyd7mZmZB/3agaZ0zTXXeIGUBJMmTfJExEtOTvbOPfdcb/r06d66desaPG7feXL48OFebW1tXfymm27yQkNDvfz8/LrYoEGDvEGDBtX9edmyZZ6IeJ06dWpw3C1YsMATEW/ZsmUH/yLheZ7n8U+9jSwuLq5Bd+9VV11V788LFiyQxMREOf3002X37t11//Xt21fi4uJk2bJlIiJ1//z15ptvSlVVlXF9SUlJUlJSIkuXLg3+iwEa2emnny7/+c9/ZMyYMbJ69Wr5xz/+ISNHjpSMjAx5/fXX6x736ytxBQUFsnv3bhk0aJD8+OOPUlBQUO85e/bsKQMHDqz7c1pamnTr1k1+/PHHutiiRYtkwIAB0q9fv3qP2/dTjF/79bqLiopk9+7dMnDgQCktLZXvv//+0N4A4AgxZ84ceeyxx6Rjx46ycOFCufXWW6VHjx4ybNgw+emnnxo8/sorr5SQkJC6Pw8cOFBqampk69atB1zX5MmT1d/JI3AUfo2suLhY4uPj6/4cFhYm7dq1q/eYjRs3SkFBgbRq1UrS0tLq/VdcXFz3o/ZBgwbJ2LFjZdq0aZKamipnn322zJkzRyoqKuqe6+qrr5auXbvKqFGjpF27dnLppZfK22+/fXheLBAEJ598srzyyiuSl5cnq1atkjvuuEOKiorkvPPOk7Vr14qIyIoVK2T48OESGxsrSUlJkpaWJnfeeaeISIPCr0OHDg3WkZycXO/3s1u3bpVjjjmmweO6devWILZmzRo599xzJTExURISEiQtLa3uB/H7rxs4khUXF0t2dnbdf7/+TV6LFi3kmmuukS+++EJ2794tr732mowaNUref/99GTduXIPn2v84TE5OFhFp8Dt2k44dOx7iK8Gv8Ru/RrRjxw4pKCiQLl261MUiIyOlRYv69XZtba20atVK5s2bZ3yetLQ0EfnlR7MvvfSSrFy5Ut544w1ZsmSJXHrppfLggw/KypUrJS4uTlq1aiVff/21LFmyRBYvXiyLFy+WOXPmyKRJk+SZZ55pvBcLBFlERIScfPLJcvLJJ0vXrl3lkksukQULFsjFF18sw4YNk+7du8tDDz0k7du3l4iICFm0aJH885//bNCQERoaanx+z/MC3qb8/HwZNGiQJCQkyF//+lfp3LmzREVFyZdffil//OMfjc0gwJFq+vTpMm3atLo/Z2ZmGufmpaSkyJgxY2TMmDEyePBg+eCDD2Tr1q11vwUUObTjkKt9wUXh14j2zUMaOXKk9XGdO3eWd999V0499dSD2sEHDBggAwYMkL/97W/y73//WyZMmCDz58+Xyy+/XER+OWGeddZZctZZZ0ltba1cffXV8sQTT8if//znekUocKTYNy5p586d8sYbb0hFRYW8/vrr9a4i7PtJhB+ZmZmycePGBvH169fX+/Py5ctlz5498sorr8hvf/vbuvjmzZt9rxtoriZNmiSnnXZa3Z8P5vx00kknyQcffCA7d+6sV/gF26//2RiB4Z96G8n7778v9957r3Ts2NH4O6Ffu+CCC6SmpkbuvffeBsuqq6slPz9fRH65JL7/34769OkjIlL3z7179uypt7xFixZ1A6R//U/CQHO0bNky4xWARYsWicgv//S678rBrx9XUFAgc+bM8b3e0aNHy8qVK2XVqlV1sV27djW4Cm9ad2VlpcycOdP3uoHmqlOnTjJ8+PC6//aNb8nOzq772cWvVVZWynvvvSctWrRo9IsM++bf7js/4uBxxS8IFi9eLN9//71UV1dLTk6OvP/++7J06VLJzMyU119/XaKioqz5gwYNkqlTp8p9990nX3/9tYwYMULCw8Nl48aNsmDBAnn44YflvPPOk2eeeUZmzpwp5557rnTu3FmKiorkqaeekoSEBBk9erSIiFx++eWyd+9eGTp0qLRr1062bt0qjz76qPTp04dWdzR71113nZSWlsq5554r3bt3l8rKSvnkk0/khRdekKysLLnkkkskJyen7qr21KlTpbi4WJ566ilp1aqV7Ny509d6b7vtNpk7d6787ne/kxtuuKFunEtmZqZ88803dY875ZRTJDk5WSZPnizXX3+9hISEyNy5c339szFwpNqxY4f069dPhg4dKsOGDZP09HTJzc2V559/XlavXi033nijpKamNuo29OnTR0JDQ+X++++XgoICiYyMrJvtCTsKvyD4y1/+IiK//BNry5Yt5dhjj5UZM2bIJZdcUq+xw2b27NnSt29feeKJJ+TOO++UsLAwycrKkosvvrjub1mDBg2SVatWyfz58yUnJ0cSExOlX79+Mm/evLofv1588cXy5JNPysyZMyU/P1/S09PlwgsvlHvuuafBbwuB5mb69OmyYMECWbRokTz55JNSWVkpHTp0kKuvvlruuusuSUpKkqSkJHnppZfkrrvukltvvVXS09PlqquukrS0tAbDzA9WmzZtZNmyZXLdddfJ//zP/0hKSor84Q9/kLZt28pll11W97iUlBR588035ZZbbpG77rpLkpOT635zeKCfdABHi27dusmMGTNk0aJFMnPmTMnJyZGoqCjp3bu3PPXUU/WOmcaSnp4us2fPlvvuu08uu+wyqampkWXLllH4HYQQj7+qAgAAOIFLQAAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOOKgBzhzXzyRpKQkY3zAgAFqzttvv91IW1Pf2WefbYy/8847ak5ZWVljbc4RozmOseRY02k3ehcRqampOYxbEpjw8HBjvKqqKuDnOuecc9Rlr776asDPp72nwX4/Odaaju11ap/LvluimYwdO9YYt90kQNsG23q0GyA8/PDDak5paWlA6xdpnvvmoTjQ6+GKHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA44qC7eiESFxdnjCcmJqo5xx9/vDG+evXqgNd/0kknqcsSEhKM8ZYtW6o5P/30U8DbAARLWJj+9VNbW2uMB7vTVOv007pwRfSOOVsnnZ/u3bPOOssYt3U0fvDBB8Z4Xl6emuNKZ+uRRvtc/HSg+snp0aOHuuyZZ54xxu+880415+STTzbGbfvfhg0bjPHu3burOV9++aUxfrR17h4KrvgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABwR4h1kjzMt/yLt2rUzxm03ji8rKzPGc3Nz1Rzt+WJiYtQc7UbXtjESe/bsUZe5ojm2+HOs6QYMGKAuW7lypTEeERGh5lRWVh7yNh2MNm3aGOMzZ85Uc4YNG2aMr127Vs15+umnjfHZs2erOVFRUcZ4eXm5muMHx1rj084d0dHRak6LFubrP4WFhWrOlClTjPE+ffqoOYsWLTLGtXOXiMjChQuN8VNPPVXN2bx5szFeUFCg5mjnycP1/RBsBzrWuOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6gqzcA48ePN8ZvvfVWNee2224zxm0dRlqXXfv27dWc6667zhifOHGimrNp0yZ1mSvoNAwO2zZr7/HYsWMDXs/VV1+tLtO6YP3o0qWLuuz66683xocMGaLmaJ2LiYmJak5KSooxPmHCBDVHe39sXZDaZ6d1e4qI1NTUqMs0R/ux5ucYsImPjzfGtf3Cth7b+sPCwoxxrTtWRD9Hvfzyy2rOQw89ZIwXFRWpOR06dAh427Rl2vspou/rts9Um9jx888/qzmHC129AAAAEBEKPwAAAGdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwhLmPG0Y7d+40xm1jVu6//35j3M84l4SEBDUnNTXVGK+oqFBzgKa0fPlyddkjjzxijD///PMBr2fatGnqMm00SkZGhprz+eefG+NPP/20mvPggw+qywJ12mmnqctOOeUUY1wbRSWiv6e2cS61tbXGeHMc2XK4+Hnt2igVEZGWLVsa49XV1QFvgzZ6RESksrLSGI+JiVFzsrKyjPGbbrpJzdH2zbVr16o5GzZsMMZt41y0fdP2voWGhqrLNG3btjXGbaOOcnJyAl5PY+CKHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gq7eABQXFxvjto4grdvWdqNtWzedJjo6OqD1i4js2LEj4PUAJraOxvDwcGN8z549ao7W6Xf22WerOX379jXGf/e736k52k3lH3jgATUnmJ151157rbpM6wT+6aef1Jzt27cb44899pias3HjRmNc615G4EJCQoxxrTtWRCQ3N9cYLyoqUnO0yQ9VVVVqjtaFapsIoe1nHTt2VHPWrFljjK9atUrN0XTu3FldtnXrVmPcdl5NTEw0xm3fURrb9I3mgit+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHMM4lAK1btw44p7y83Bi3tZZr4y+0kQAieku+n9EwQDBpN023ef75541x2+iHv/zlL8a47ebs2pgTPyNb/v73v6vL7rjjjoCfT7vhve0m8LfddpsxbvsMXnvtNWO8a9euak5JSYm6DA1pI79sY5C0z+zYY48NeP0RERHqMm38yXHHHafmfPvtt8a47Rzl5/w5btw4Y/yrr75ScwYMGGCM79q1S83Rtm337t1qTkZGhjGenp6u5mjvtW0/aAxUBQAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCLp6A3DiiSca47ZOJq2r1tZlpeXYuvm0TuDjjz9ezfnyyy/VZUCw2I4PTZcuXYzx9evXqzm33HKLMf7II4+oOevWrTPGv/vuOzUnJibGGA8L079Ou3XrZoxv2LBBzdHcd9996rLo6GhjvKysTM2Jj483xrdt26bmpKSkqMvQUHJysjHet29fNadVq1bGuG3f/OSTT4xx7TMWEWnXrp0xbuuG1zqObVMkIiMjjfEOHTqoOUlJSca4di4WEVm9erUx3r17dzVnz549xnhmZqaao30OtmNjzJgxxrjWWd9YuOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAE41wC0LlzZ2PczzgXG200i22ci7YNJ5xwgpozZ86cwDYM8EEb/WCzdOlSY9x2M/Orr77aGNfGYojoIytsx3RUVJQxHhoaqubcddddxviMGTMC3jbbWAptTJRtnEtBQYExbnvfxo8fb4w///zzao7LtBEfe/fuVXOys7ON8crKSjVnwoQJxnhVVVXA6znllFPUHO35bMdNRUWFMW4bbaaNSPriiy/UnK5duxrjq1atUnO01zp8+HA1JyEhwRhv06aNmrNp0yZ12eHEFT8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcARdvQHQboBu6zTUupxsHYBaV295ebll68zat28fcA4QTFpnu63bNy0tzRi/77771JzRo0cb42vXrlVztJumazdgFxGJi4szxrXOXRGRm2++2RifOHGimqN9r+zYsUPN0TqBte5IEf1zsHWCnnvuucY4Xb1mkZGRxvjXX3+t5pSWlhrjJ554opqzbds2Y3znzp1qzvbt241x236Wnp5ujCclJak5ixYtMsZt++bChQuN8V69eqk52rGrfaeI6N22Z599dsDrsXUca+/b4cYVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIxjnEoDk5OSAc2w3rdZo4y+0uIhITU2NMZ6RkRHw+oFgsu23Gu1YGzRokJqjjbIYNmxYwOu3KS4uNsZvv/12NUdbFhUVpeY8+OCDxvi4cePUHO17ICIiQs3RxrbYxu20bNlSXeaqxMREdVl8fLwxbht/kpqaaozv2rVLzdE+/6KiIjVHG09kGwFTUFBgjCckJKg5Xbt2NcZtY4O0kTZbt25Vc2JjY41xbZtF9PfA9r5t3rzZGM/KylJztJFs2rgfEZGKigp1mV9c8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR9DVGwCtW0frpBIRCQ0NNca1G7CLiISHhxvjtu5I7ebsHTt2VHOAw8HWHarp1KmTMf7uu++qOT179jTGf/zxRzXnpptuMsZfe+01Nad3797G+AUXXKDmnH766cb4gAED1Byto9HWnah1B9q+O7TJA9p3l4jITz/9pC5zle27VpuuUFZWpuZo3eOZmZlqjtbtqnW62rZB63QVEYmOjjbGbec1raO1pKREzdG60cPC9NJF65Rv1aqVmqN1ZGvbbFvWpk0bNUfrbLZ193/88cfqMr+44gcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcATjXAKQnp5ujNva0bVRCba2d9sYBY02UsbWwg4cDtqoIZv+/fsb42+99Zaao42lOOGEE9SchQsXGuN79+5VcyorK43x7OxsNWf37t3G+L/+9S81RxtlMWLECDVHYxvnon0X2cZ5zJo1K+BtONp9/fXX6rI1a9YY4507d1ZztHNHv3791BxtbMvGjRvVnJYtWxrjtnOUtm3asWGTkJCgLtNG2qSkpKg52vioXbt2qTnaub179+5qjja+xzbe7aOPPjLGN2zYoOY0Bq74AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjQjxb686vH6h08bhEe6t27NgR1PWkpqYa47ZOQ+3zsd0wms/U3rnWVI7Ez8XWia51uU2ePFnNueiii4zxdu3aqTkdOnQwxhcsWKDmfPjhh8b4pEmT1Jxjjz3WGE9LS1Nz/OjSpYsxvnLlSjVH+xxs3ZYxMTEBr+f0009Xl2k41oLDts2DBw82xiMjI9UcrXPWJizMPBDETyewreu/tLTUGNeOdRG9s3nr1q1qTtu2bY1xretfRKSqqkpd1tQOdKxxxQ8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHEHhBwAA4AhzT7bDtFEqwWa7abptWTCFh4cb4825TR3Nk5+xGNrN1EVE+vbta4w/9dRTas4dd9wR8DZonn76aXXZihUrjHHbCAXt9Xz55Zdqzvvvv2+M225Qv2fPHmO8rKxMzYmKijLGL774YjUHDfk5BvyMuLHlJCQkGOO2bdPGLdlytPEwtnOHth5t/xPRvyO01ykikpiYaIzn5uaqOa1btzbGj9ZzIVf8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARdPXux3YTeI2fG1PbaDl+OrNsOnXqZIyvX78+4OcCAvXee++py15//XVj/KqrrlJzsrKyjPHx48cHtF0Hcuqppxrj//jHP9ScJUuWGOO2TkOt617rEBYRuemmm4xxW4eudiP6nJwcNQcN+enQDbaYmJiAc7TO1erqajXHzzkqLMxcbmj7uYhIbGysMW7rbNe6erXnEhGJi4tTlwXK9h40h31EhCt+AAAAzqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHMM5lP6mpqQHn2Fq0Q0NDjXHbDeq1dvAWLfQ63c84F238BeNcECjb6AfN1KlT1WWXXnqpMf7++++rOY8//rgx/s0336g5xx13nLosULfddpu6bPLkyca4NuZFROTmm28OeBteeuklY9w2zmX27NkBr0f7jmou4ypcFR8fb4zbRoxo5xVtzIuISGRkpDFeUVGh5mjnKO0cKaLvTwkJCWqONpolKipKzdHet6MVV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBF09e4nLS0t4BxbR6N2Y2obP129tg4sTevWrQPOQdOxdb/56eq2PZ9Gu9F5YWGhmtOtWzdjfOXKlWrOsGHDjPHnnntOzdGWbd++Xc35+eefjfG2bduqOZpPP/1UXfbiiy8a4346d20yMzMDzrF1Smu07zU/30MInvDwcGPcdu7w8z0QzMkTtn1G27aIiAg1R/uOSkpKUnNiYmLUZUcjrvgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABzBOJf9pKamBpzj58bktrZ327JgSkxMPCzrQXAEez/TRj/YbrRuG9uiGTRokDG+c+dONae2tjbg9Wjat2+vLlu/fr0x/t5776k53377rTFuez3XXXedukyjjbKwje4pKyszxgsKCtQc2zgqjZ99EY1P22dsI1u0Y812DGrjXLS4iP59Yxvnoo0NioyMVHO0US+2cS5+joEjGVf8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARdPXux8/Nmm0dblpnlK3LSuuMsq3HT5ed7UbXaH5sXXZa964tp7y8POBtOO2004zxadOmqTlt27Y1xt966y01Z8GCBcb4SSedpObk5uYa49u2bVNzunXrZox/8sknas7UqVONcT/fHbbuRO2zs3X1ah3ZJSUlgW3YAdi2Ac1PVFRUwDl+zlG2HK1zVuvcta3H1gmsLbN19bq2P3PFDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCMa57MdP27vtxtSHK8fP89lGfaD50W5yLuJvHIE2gmXIkCFqTkZGhjFuO260/WzgwIFqTlZWljG+bt06NWfjxo3GeE5OjpqzZMkSY3z79u1qzjfffGOM20YqaSMrtPErIiLR0dHGuG2UhZ+xPjh6aPuGNkpFRP/u8DNmxc9YMdu2aaOgSktL1RztuLGNcyksLFSXBcrPe3C4ccUPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxBV+9+EhISmnoTVH46d21sN9RG82Pr5tT06dNHXXbbbbcZ4z///LOaU1JSYox/8cUXas6oUaOM8djYWDVH6+o99dRT1Rytm27ixIlqzplnnmmMd+/eXc05/vjj1WUaP51+fnK07l3XbkJ/pLB9p/v5/LXOVdtzafuMbdu0CQOVlZVqjvZ8tn0zJibGGC8vL1dztGWJiYlqTkREhLosUMH+TBsDV/wAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI5gnMt+/Iw4sbWjazdN98PWJm67obYmLi7uUDYHh1nv3r3VZevXrzfG//jHP6o5BQUFxnh+fr6ac+KJJxrjq1atUnOuuuoqY3zu3Llqjm2kjEa7OfvLL7+s5mjjJz788MOA16+NuBDxN4rHD21cRHMZI4HGFRkZaYxXVFSoOdq5w3a+qa6uDiguop9bbedP7Ziy5WivVRtbc6DnOxpxxQ8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHEFX7378dL/56ai1dUz5WY+2rLS0VM3RboCN5unOO+9Ul2kdv1qXn4hIcXGxMb579241R+uCnThxoppz+eWXB/RcIiIvvPCCMW7r9tVuzm7rNExPTzfGH3/8cTXncPHzHYEjS7C7rbXOWT/rsR2f2veK7ftG62y3HZ/aNkRFRak52veA7XhKTU1Vlx2NuOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAE41z2ExERoS7Tbv5sG4uiPZ/Wdi+it7fbWti1G1Dbti05OVldhubnww8/VJeNHz/eGM/OzlZztP0sMzNTzdFGvdiOm7y8PGP8+++/V3N27dpljLdoof9dVRtZkZCQoOZ88803xviyZcvUHI1tLEVTs92gHkcP7XO2jTIJDw83xm0jYLQc7Rwp4u/4KCkpMcYTExPVHG20me09iIuLC2zDLGzrCfb4Hr+44gcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjqCrdz+rVq1Sl/Xu3dsY37p1q5qTn59vjJ9yyilqTk1NjTGudSCK6J1MHTt2VHM++eQTdRman9mzZ6vL+vTpY4wPHDhQzcnIyDDGU1JS1Byta0+7AbuI3snWvXt3NUe70bof8fHx6rIFCxYE/Hxad30wt9kv7XvA9h5o/HRQIzDB7gDV9s2ysjI1RzumbZMntBxbV692XrN9dxQUFBjj2n4uou/rtm3r1KmTuuxoxBU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjGOeyH21ki4jedh4dHa3maO36rVq1UnN27NhhjFdWVqo52s25bSMZMjMz1WU4svzhD38IOOfss882xi+44AI155xzzjHGi4qK1BxtxITtuElLSzPG/YyNWbt2rZpz//33q8s0tuMwmLTxFzbbt283xt99992An0v7ThGxjyHBwQv2WJy8vDxjvGfPnmpORESEMW7bNm3Ui22kkW1/0rRt29YY10ZRiYgkJSUZ4wkJCWqObdRLoI6EUUdc8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR4R4B9mCQheXSExMjDGenJys5hQWFhrj/fr1U3P27NljjK9Zs0bN0W5Mbetk2rJli7rMFc2xA0s71oJ9Q/fDpUuXLsZ4amqqmhMXF2eM2242X1xcbIyvXr3asnVmtm54P92Jfmifd3P+rG2a43a7cl7LyspSl2ldsGFh+tAP7ZxXXV2t5tg68jVa57+ts147f2od7yIiBQUFgW2YRXP4nj7QerjiBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwxEGPcwEAAMCRjSt+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAICj8AANAonn76aQkJCZHPP//8gI8dPHiwDB48uPE3ynEUfoqQkJCD+m/58uVNvanAEY1jDTj8DvW4q62tlWeffVb69+8vLVu2lPj4eOnatatMmjRJVq5c2ejbv3btWrnnnntky5Ytjb6uo01YU29AczV37tx6f3722Wdl6dKlDeI9evQ4nJsFHHU41oDD71CPu+uvv14ef/xxOfvss2XChAkSFhYm69evl8WLF0unTp1kwIABAW/TO++8c9CPXbt2rUybNk0GDx4sWVlZAa/LZRR+iosvvrjen1euXClLly5tEN9faWmpxMTENOamNYqSkhKJjY1t6s2AgzjWgMPP73EnIpKTkyMzZ86UK664Qp588sl6y2bMmCG7du3ytU0REREHfEx5eflBPQ46/qn3EAwePFh69+4tX3zxhfz2t7+VmJgYufPOO0VEJDc3Vy677DJp3bq1REVFyfHHHy/PPPNMvfzly5cbL6Vv2bJFQkJC5Omnn66LZWdnyyWXXCLt2rWTyMhIadOmjZx99tkNLnMvXrxYBg4cKLGxsRIfHy9nnHGGrFmzpt5jpkyZInFxcbJp0yYZPXq0xMfHy4QJE4L2vgDBxrEGNB+bN28Wz/Pk1FNPbbAsJCREWrVq1SBeUVEhN998s6SlpUlsbKyce+65DQrE/X/jt++4nT9/vtx1112SkZEhMTEx8sgjj8j5558vIiJDhgzh5yAB4orfIdqzZ4+MGjVKxo0bJxdffLG0bt1aysrKZPDgwfLDDz/ItddeKx07dpQFCxbIlClTJD8/X2644YaA1zN27FhZs2aNXHfddZKVlSW5ubmydOlS2bZtW91l7rlz58rkyZNl5MiRcv/990tpaanMmjVLTjvtNPnqq6/qXQ6vrq6WkSNHymmnnSbTp08/Iq+cwC0ca0DzkJmZKSIiCxYskPPPP/+g9unrrrtOkpOT5e6775YtW7bIjBkz5Nprr5UXXnjhgLn33nuvREREyK233ioVFRUyYsQIuf766+WRRx6RO++8s+6fo/k5yEHycFCuueYab/+3a9CgQZ6IeLNnz64XnzFjhici3nPPPVcXq6ys9H7zm994cXFxXmFhoed5nrds2TJPRLxly5bVy9+8ebMnIt6cOXM8z/O8vLw8T0S8Bx54QN2+oqIiLykpybviiivqxbOzs73ExMR68cmTJ3si4t1+++0H/fqBw4VjDTj8TMedzaRJkzwR8ZKTk71zzz3Xmz59urdu3boGj5szZ44nIt7w4cO92trauvhNN93khYaGevn5+XWxQYMGeYMGDar7877jtlOnTl5paWm9512wYIHxmMaB8U+9hygyMlIuueSSerFFixZJenq6jB8/vi4WHh4u119/vRQXF8sHH3wQ0Dqio6MlIiJCli9fLnl5ecbHLF26VPLz82X8+PGye/fuuv9CQ0Olf//+smzZsgY5V111VUDbATQljjWg+ZgzZ4489thj0rFjR1m4cKHceuut0qNHDxk2bJj89NNPDR5/5ZVXSkhISN2fBw4cKDU1NbJ169YDrmvy5MkSHR0d1O13Gf/Ue4gyMjIa/NB069atcswxx0iLFvXr6n2XoQ9mR/+1yMhIuf/+++WWW26R1q1by4ABA+TMM8+USZMmSXp6uoiIbNy4UUREhg4danyOhISEen8OCwuTdu3aBbQdQFPiWAMOr+LiYikuLq77c2hoqKSlpYmISIsWLeSaa66Ra665Rvbs2SMrVqyQ2bNny+LFi2XcuHHy0Ucf1XuuDh061PtzcnKyiIj6F6xf69ix46G+FPwKhd8hOpS/hfz6bz+/VlNT0yB24403yllnnSWvvvqqLFmyRP785z/LfffdJ++//76ccMIJUltbKyK//PZo3wnq18LC6n/UkZGRDU6WQHPGsQYcXtOnT5dp06bV/TkzM9M4Ny8lJUXGjBkjY8aMkcGDB8sHH3wgW7durfstoMgvRaOJ53kH3A6u9gUXhV8jyMzMlG+++UZqa2vrfeF///33dctF/v/fePLz8+vla1cpOnfuLLfccovccsstsnHjRunTp488+OCD8txzz0nnzp1FRKRVq1YyfPjwYL8koFniWAMaz6RJk+S0006r+/PBFGAnnXSSfPDBB7Jz5856hV+waX+Zw4Hx19BGMHr0aMnOzq7XrVRdXS2PPvqoxMXFyaBBg0Tkl5NSaGiofPjhh/XyZ86cWe/PpaWlUl5eXi/WuXNniY+Pl4qKChERGTlypCQkJMjf//53qaqqarBNfucqAc0ZxxrQeDp16iTDhw+v+2/f+Jbs7GxZu3Ztg8dXVlbKe++9Jy1atJAuXbo06rbtm4W5/1/mcGBc8WsEV155pTzxxBMyZcoU+eKLLyQrK0teeuklWbFihcyYMUPi4+NFRCQxMVHOP/98efTRRyUkJEQ6d+4sb775puTm5tZ7vg0bNsiwYcPkggsukJ49e0pYWJgsXLhQcnJyZNy4cSLyy++KZs2aJRMnTpQTTzxRxo0bJ2lpabJt2zZ566235NRTT5XHHnvssL8XQGPiWAMOvx07dki/fv1k6NChMmzYMElPT5fc3Fx5/vnnZfXq1XLjjTdKampqo25Dnz59JDQ0VO6//34pKCiQyMhIGTp0qHGGIOqj8GsE0dHRsnz5crn99tvlmWeekcLCQunWrZvMmTNHpkyZUu+xjz76qFRVVcns2bMlMjJSLrjgAnnggQekd+/edY9p3769jB8/Xt577z2ZO3euhIWFSffu3eXFF1+UsWPH1j3uoosukrZt28r//M//yAMPPCAVFRWSkZEhAwcObNANCRwNONaAw69bt24yY8YMWbRokcycOVNycnIkKipKevfuLU899ZRcdtlljb4N6enpMnv2bLnvvvvksssuk5qaGlm2bBmF30EI8Q7ml5UAAAA44vEbPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHHHQA5y5L57I6aefbozbRiFq08t/+OEHNeeLL74wxidMmKDm5OTkGOO2eytu2LDBGN93n1MXNMcxlhxrIlOnTjXG586dq+aUlZUZ47ZjoLS0NLANk1/uGGDSpk0bNWfx4sUBr+dow7HW+PbdXWZ/f/nLX9ScWbNmGeOPPvpoULapMRx//PHqskceecQY37Ztm5pz2223GeM7d+4MbMOaiQMda1zxAwAAcASFHwAAgCMo/AAAABxB4QcAAOCIEO8gf3F7tP0IVjN+/Hh12b///e+grae2tlZd9tlnnxnj/fv3D3g9hYWF6jLtM01ISAh4PUcqfnDedEaMGKEue/HFF43xn376Sc0JCzP3qiUnJ6s5W7duNcZDQ0PVnJiYGGM8JSVFzZk4caIx/vbbb6s5RxuOtYZGjRqlLrvzzjuN8d69e6s5xcXFxnh2draak56eboxnZGSoOXl5ecZ4bm6umlNUVGSM246bTp06GeO2pqy1a9ca4+Hh4WpOhw4djPEdO3aoOfPmzTPGZ86cqeZo70Gw0dwBAAAAEaHwAwAAcAaFHwAAgCMo/AAAABxB4QcAAOAICj8AAABHMM5lP2effba67NVXXzXGf/zxRzUnIiIioLiI3pKvjasQ0du3Kysr1ZxjjjnGGI+KilJzKioq1GVHIkZMNJ033nhDXda1a1dj3LY/a/fkXbdunZqjff5VVVVqjrYsNjZWzdHuf/1f//Vfas7R5mg51rQc2+vr27evMa6N7hLR79men5+v5pSXlxvjtu90bcSIn/En2vgVEX20mG3bPv/8c2N89+7dak58fLwxbvusa2pqjHHbqBltme08rX2vBRvjXAAAACAiFH4AAADOoPADAABwBIUfAACAIyj8AAAAHKG3nzjK1pXkh9bho3URidg7fgNl65jS2LqSjrauXjSdPn36qMtsnYuaFi3Mf4/NzMxUc7QbutfW1qo5fo6p4447LuAcNE9+upPnzZtnjGuduyIiu3btMsZjYmLUHK1z1Xa+SUpKMsa1bl8RkQ0bNhjj33zzjZqjvW/V1dVqTsuWLY1xbZtF9O8B2+emnXO1CRsi+vuTkZGh5owfP94Yf/7559WcxsAVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIxjnsp/+/fury7R2cNvNn7XWci3udz1aTllZmZqj8TP+AtBoI5JsIxl27NhhjMfFxak52dnZAT2XiEhCQoIxbhvjoI3TKC8vV3O0G8f37t1bzfnuu+/UZWh+bGNWOnfubIzbxrlo+3pVVZWao50jbOeOyspKY9z2erRxR126dFFztNezfv16NUc7bkpLS9Ucje2cq427CQ0NVXO0ZbZRUFOmTDHGGecCAACARkHhBwAA4AgKPwAAAEdQ+AEAADiCwg8AAMARdPXuJzExUV1WUVER8PNpHVPh4eFqjq0rSKN1LNm6uTTajbEBP3r16mWM5+bmqjl5eXkBr0c7drdt26bmaMdNdHS0mqN1yts6ALWctLQ0NQdHFq17XUTfz2zf9WFh5tOzLUfrTg32FAltf7Z13aenpxvjX331lZoTGRmpLtNo74/tfdPeH9sxrb3XWlxEpGvXruqyw4krfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAARzDOZT+29m2Nre1daxOPjY0NeBtsN6bWbtytterb2Fr/gUBp+3NCQoKa0759e2P8559/VnO057Mdn9q2lZeXqznaiKaIiAg1R9sG7blw5LGNc/FDG/llGxeifXfbcrRzhHZOEdH3ddu5IyoqSl0W6Hpso1mqq6sDXo8f2uga2zlXG5UWHx+v5hQVFQW2YQeBMzwAAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOIKu3v0Eu2NK6zTcsmWLmvPhhx8a41dffbWas2PHDmPcT4duRUVFwDmApmPHjsb43r171Rw/XYN+OvK1TkNbB6LWWWzrvmvbtq0xbusEXbFihboMzU+HDh3UZdnZ2ca4bX8OCzOfnm37uXYusnW2a+uxbVtZWZkxbps8EcxuW1s3vPZ6bF3KGluHrnZuLy4uVnO077Vu3bqpOZ9//rm6zC+u+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAIyj8AAAAHME4l/1obeoiehu97YbRsbGxxvjOnTvVnD/96U/GuG2ci0a70beNrR0dCFRWVpYxbhuVoI2fsI2YKCgoMMZjYmLUHO35ysvL1ZwuXboY4z/88IOao42uiYyMVHNwZOnRo4e6TBtlYjt3aPutbX/WxnrZzgPa+C7bCBhtG7SxNSIi+fn5xnhiYqKao42n0Ua22LbNdl5r2bKlMX7ssceqOdprtdUQ2n7Qrl07NYdxLgAAAPCNwg8AAMARFH4AAACOoPADAABwBIUfAACAI+jq3U9hYaG6TOvqtd18WuuMWr16tZqjdT/ZaF1bfm5MvWfPnoBzAM1XX31ljJ955plqjrbf2o41rQMwKipKzWnfvr0x/vXXX6s52vFp67bUltm6E3Fk6d69u7pM228jIiLUnLZt2xrjP//8s5qjddDbvtP97IPasVZZWRnwcyUlJanLtM5/7Vxs06ZNG3WZ1l199913qzkPPPCAMW7rwtW6rk8++WQ159VXX1WX+cUVPwAAAEdQ+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAI5glsJ9du3YFnGNrydf8+OOPAefYaC35fsa55ObmHurmAHW00Q+2m7OvW7fOGI+MjFRztGW28QraaIwLL7xQzVm6dKkxXlJSouZo41z8HJ9onrp06aIu00YAdevWTc1ZtGiRMX7HHXeoOT/88IMxXlpaquZobKOTtP3Wz2gY22gWbUyZ9p1i24aWLVuqOXl5ecb4P//5TzVHG+cSHR2t5mjjbnr37q3mNAau+AEAADiCwg8AAMARFH4AAACOoPADAABwBIUfAACAI+jq3U9hYeFhWc8XX3wRcM6mTZvUZa1btzbG/XRzaTesBvz46quvAs7Rbipv64LVuvZ27typ5jz88MPG+MSJE9Wc+Ph4Y7ysrEzN0ToXQ0JC1BwcWWxd6nv37jXG4+Li1Jz33nvPGLdNhND2J1sXrNaFasvxPC+guG3bbLRzka27P9Dn8kubCBAbG6vmaPtBVlZWMDbpoHHFDwAAwBEUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCMa57KegoCDgnBYtAq+fd+zYEXCOduN6EZE2bdoY47YWettNuIFgWb9+vTGu3bBcRB+JkJ2dHfD6tRuwi9hHJGm00Sy2m81ry4qKigJeP5pW+/btjXHbiBE/37V+9g1tnIpt27SxLbZt1kaj2M432rbZxsZoKioq1GXatiUlJak5u3btCngbtNEsrVq1UnNyc3ONcW2faixc8QMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR9DVux/t5vA2YWGBv422G8drtm/fHnCOreO4tLQ04OcDAlVVVWWMFxYWqjnh4eEBryciIsIY93Os2b4HEhMTjXHtpu0ielevnykCaFpt27Y1xm3fz34mP2gdoDZaJ67W6Sqid8hqx61tPX5ep62r17bdGtt2a/yc91esWGGMT5o0Sc3RXmtMTIyak5WVZYxv2bJFzTkQrvgBAAA4gsIPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABzBOJf9FBcXB5zjZ/SEdsNqG1t7v7YNtvb6kpKSgLcBCBbbOAJtZIpt7IFm69atAef8+OOP6jLthvda3LbMz83h0bTGjh1rjEdFRak55eXlxrhtpNYPP/wQ2IaJPv4kJCREzdGW2c4dfs432rbZcvxsmzaeRhv3JCKSn5+vLtPMmTPHGB83bpyao30P2EZBnXfeecb49OnTLVtnxxU/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6g8AMAAHAEXb370bqvRESKioqMce0G7CIie/fuPeRt2sfPzebDwvSPWHs9wOGwY8cOdVlKSkrAz6d1yvvZz23HWufOnY1xP92JOTk5gW0Ympy2n9m6utPS0oxxW5e6n2507fxl2ze185dtWkUwu4dttO22nde098D2enbv3h3YhonI559/boy3bNlSzdG+izZv3qzmaBMODgVX/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjmCcy35qamrUZVoLu43tJtyBKikpUZf5aa8vKys75G0C/CooKFCXaWMcbGMpNH72c9t4h2OOOcYYt30/VFdXG+OFhYWBbRia3B//+MeA4iIiqampxvjAgQODsk37aPugn2NAG1sjou/PtvOnn2NXU1VVpS7Tzrm24zOYY9f69u2rLsvNzTXGbaOtGgNX/AAAABxB4QcAAOAICj8AAABHUPgBAAA4gsIPAADAEXT17sd282ety8nWObtnz55D3qZ9/GybrTPL1iUMNDZbx7t243gbrdPQT2e9ny4/W9dgRESEMR4ZGRnwenDk0brEFy5cGPBzxcXFBZxj2ze1Y812Xgtmh26wRUVFGeMVFRVqTrt27YK2/i+//DJoz9VYmu+nBwAAgKCi8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAARzDOZT+20Q/aaBRba3swx7nYbrRtG9vi5/mAxubnWLONNNKWaaNUbLTxGwfaBk1BQYExro2ewJHHNv5EG5li25fKy8uN8fT0dDVHGw9UVVWl5mjnL9s5xc/4MD/j0PzQ3mvbiKaOHTsGbf22z1R7rdooKhs/5/x9uOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6gq3c/P/30k7rMT1ev7cbQgdq8ebO6TNu28PBwNcfPzeuBYLHdON5Px1owu3o3btyoLtM6J23HmrYN2nPhyGPbZ7WuzZqamoDXM3ToUHVZZWWlMW6b4KDtt7Zt83N8Brt7V6Ntm+096NGjR9DW76dD1/beHEr3roYrfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAARzDOZT+2Gzlr7e22mzL7adfXaDd6F9HHxti2raio6JC3CfBLGz0hoo9Isu3PP//8szG+Y8eOwDZMRH788Ud1mZ8RTbGxscZ4YmJiwM+Fo4efUR0XXXSRukw7R9iOG22skm3ckm2ZJpjjXGzPpY1Iso1zycrKMsb79++v5nz66acBb1tjjGbxgyt+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOAIunr3k5eXpy7Tbr6sdeyJiFRVVR3yNu1j6+rVOiRtN4GnqxdNyc9N4LUbyouIxMXFGeNah7CI3p24Z88eNeeYY44xxjds2BDwerRtxtElNDTUGLcdA6mpqcb48ccfr+Zs2rTJGLcdN9qxZutO1Y4pW9dqMDta/TyX7b3WOvUvvfRSNUfr6m0unbs2XPEDAABwBIUfAACAIyj8AAAAHEHhBwAA4AgKPwAAAEdQ+AEAADiCcS4BKC8vN8aTkpLUHNsIlkBlZ2ery/xsG+Nc0JRKS0vVZdooCdtIBu35/NxQPicnR12mbVtUVJSao411Sk9PD2zD4IyTTz7ZGE9ISFBziouLjfGYmJigbNM+2jFgGwGjHYe2nEDXbxMWppc7e/fuNcaHDBkS8HpstO0+3CNguOIHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI6gqzcAWkeh7Sbwh6tz1k/HVGVlZWNtDnBAWqeriP2Y0pSUlBzK5tRj2zatczIxMTHg5wt2tyWaJ1s3uqZ3797GuLb/iYhERkYa47bjSTt32HL8dKHaumqDSds27b0R0d/TjIwMNad9+/bG+Pbt29Wc8PBwY/xwn4u54gcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcATjXAKgteSHhoaqOYdrnIs2tsXWkm8bWQE0Nm2MhIi/Y8025iKYtGPatm3a6/Ez5gNuOOGEE4zxnTt3qjnaecB2rGnLbPuzH9qYFT+jYWxjyrTns70ebZyKbQRN69atjXHbOJfq6mp12eHEFT8AAABHUPgBAAA4gsIPAADAERR+AAAAjqDwAwAAcARdvQGoqKgwxm3dQvn5+Y20NfVpnUy2rsHy8vLG2hzggGz7n58OwLy8vEPepoNRUlIScI7WOVlQUHCom4OjVHx8vDFum8agLYuIiFBztGNK63QV0c95tikSts7iYNJej60TWGPb5piYmICfT3t/Dtd7U7cdh3VtAAAAaDIUfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCMa5BEC7YbOtTTwuLi5o67etJyoqyhiPjIxUc5KTkw95mwC/cnNz1WXR0dHGeEpKiprz7bffHvI2HYyysjJj3DbeITY21hi3jczAkcX2/WwbQ6Q59thjjXHb6A9t5JhtzIo28su2zdo22LZN2wbbevyMddI+B9toM9v7o0lPTw84x89+0Bi44gcAAOAICj8AAABHUPgBAAA4gsIPAADAERR+AAAAjqCrNwAbNmwwxm0dQcuXLw/a+m0dQYsWLTLG+/fvr+YsXbr0kLcJ8OuDDz5Ql82aNcsYz8zMVHMef/zxQ96mg/Ff//Vfxvif/vQnNaekpMQYX7FiRVC2CU0v2F29e/fuNcZzcnLUHO1cZNs2LcfWpa6pqqoKeD2hoaFqjrbMlqMpLS1Vl1VXVxvjWpe0iD5Jw8bW9Xw4ccUPAADAERR+AAAAjqDwAwAAcASFHwAAgCMo/AAAABxB4QcAAOCIEK+53DUYAAAAjYorfgAAAI6g8AMAAHAEhR8AAIAjKPwAAAAcQeEHAADgCAo/AAAAR1D4AQAAOILCDwAAwBEUfgAAAI74f+Mls9gvs03zAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A custom Dataset class must implement three functions: `__init__`, `__len__`, and `__getitem__`. Take a look at this implementation; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`."
      ],
      "metadata": {
        "id": "IfDqF55Fae2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "  def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path) # read_image converts it to a tensor\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "8QVzNdGcapyH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "Q7y7LTuVcYNZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "xTTq61W9d-Kb",
        "outputId": "6ebd773e-b4d9-437d-e811-f4512dd3f06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforms"
      ],
      "metadata": {
        "id": "fYaHDYObgRFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ToTensor converts a PIL image or NumPy ndarray into a FloatTensor. and scales the image’s pixel intensity values in the range [0., 1.]"
      ],
      "metadata": {
        "id": "bJHkGaNxg2K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) # WTF is scatter??\n",
        ")"
      ],
      "metadata": {
        "id": "cnzHPXAIghIU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the Neural Network"
      ],
      "metadata": {
        "id": "VsDlp30mi35_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "1STLaFl_i3hz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "adDqdFPLjrCE",
        "outputId": "c0150d3a-14ce-480d-cf2e-37aa672e8d20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "znzEPLaCAJn_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "roOhRuHXC46M",
        "outputId": "6bfa1ea9-f6cf-421c-fdac-ad3dfdbe0d45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "id": "8pXqL12NDF6c",
        "outputId": "0fa51347-0dd5-4476-a6be-1392f76821b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([0], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "id": "Ovq3iRsVDd1k",
        "outputId": "64bed165-268f-4024-ca2d-0e39fdba5995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "metadata": {
        "id": "uQNeD9PnDrLb",
        "outputId": "78ee4bb1-c768-4bf1-960e-ed0125c36615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "metadata": {
        "id": "wyWwz3FGD2Ml",
        "outputId": "e4c3400f-037a-4b57-f10c-4051db1854da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "metadata": {
        "id": "kZ0IIo1TEjNr",
        "outputId": "149a04c0-e558-467b-fd02-dd6954334dce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[-0.2716, -0.2568, -0.0175,  0.1688,  0.1726, -0.3749, -0.3573,  0.0033,\n",
            "          0.4846,  0.3244,  0.1737,  0.4314, -0.4266,  0.0400, -0.3818,  0.3704,\n",
            "         -0.3334, -0.1814, -0.0052,  0.3178],\n",
            "        [-0.5116, -0.1929,  0.2952,  0.2569,  0.2130,  0.0588, -0.2313, -0.1251,\n",
            "          0.4789,  0.5708,  0.2790,  0.5261,  0.0967, -0.0224, -0.3544, -0.1599,\n",
            "          0.1021,  0.0081,  0.1373,  0.4021],\n",
            "        [-0.5477, -0.2597,  0.0312,  0.3370,  0.2136, -0.3204, -0.4219, -0.1094,\n",
            "          0.3455,  0.4944,  0.2293,  0.4705, -0.0645,  0.2073, -0.5701,  0.1007,\n",
            "          0.0606, -0.2737,  0.0814,  0.8076]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.1688, 0.1726, 0.0000, 0.0000, 0.0033, 0.4846,\n",
            "         0.3244, 0.1737, 0.4314, 0.0000, 0.0400, 0.0000, 0.3704, 0.0000, 0.0000,\n",
            "         0.0000, 0.3178],\n",
            "        [0.0000, 0.0000, 0.2952, 0.2569, 0.2130, 0.0588, 0.0000, 0.0000, 0.4789,\n",
            "         0.5708, 0.2790, 0.5261, 0.0967, 0.0000, 0.0000, 0.0000, 0.1021, 0.0081,\n",
            "         0.1373, 0.4021],\n",
            "        [0.0000, 0.0000, 0.0312, 0.3370, 0.2136, 0.0000, 0.0000, 0.0000, 0.3455,\n",
            "         0.4944, 0.2293, 0.4705, 0.0000, 0.2073, 0.0000, 0.1007, 0.0606, 0.0000,\n",
            "         0.0814, 0.8076]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "metadata": {
        "id": "AOzWV-YnEzh4"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. `dim` parameter indicates the dimension along which the values must sum to 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "50HjlXnuGCSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ],
      "metadata": {
        "id": "0cJjpWWYGIfZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "id": "wTSOS0BGG0a0",
        "outputId": "de532aa3-0d5f-48e3-dee6-9678d878f8b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0256,  0.0052, -0.0271,  ...,  0.0196, -0.0110,  0.0013],\n",
            "        [-0.0350, -0.0351,  0.0178,  ...,  0.0216, -0.0064,  0.0058]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0351, -0.0003], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0402,  0.0244,  0.0047,  ..., -0.0260,  0.0016, -0.0321],\n",
            "        [ 0.0333, -0.0385, -0.0160,  ...,  0.0423, -0.0061,  0.0124]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0436, -0.0310], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0274,  0.0270,  0.0189,  ..., -0.0139,  0.0144,  0.0435],\n",
            "        [ 0.0335,  0.0398, -0.0026,  ...,  0.0282,  0.0334,  0.0060]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0301, 0.0427], device='cuda:0', grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Differentiation with `torch.autograd`"
      ],
      "metadata": {
        "id": "eczWUwzNHPPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ],
      "metadata": {
        "id": "JBGHgBv8HNcz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this network, `w` and `b` are **parameters**, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the `requires_grad` property of those tensors."
      ],
      "metadata": {
        "id": "KGRXoGevIH3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "metadata": {
        "id": "eVrpUXfTIXh7",
        "outputId": "e0231879-1db8-4037-c61c-b20002712787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x789d8ae36e60>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x789d8ae37940>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "GDbwoTK7IpTk",
        "outputId": "ff75c70b-2920-4952-d5a4-bb66a438a592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0570, 0.0536, 0.2014],\n",
            "        [0.0570, 0.0536, 0.2014],\n",
            "        [0.0570, 0.0536, 0.2014],\n",
            "        [0.0570, 0.0536, 0.2014],\n",
            "        [0.0570, 0.0536, 0.2014]])\n",
            "tensor([0.0570, 0.0536, 0.2014])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available."
      ],
      "metadata": {
        "id": "8oNM2YKRI9hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:"
      ],
      "metadata": {
        "id": "gT-aRLJwJIRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "id": "i-uYzuPpI1zO",
        "outputId": "d07e82ed-e8c6-4609-abf0-b39fe572a4b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are reasons you might want to disable gradient tracking:\n",
        "\n",
        "\n",
        "*   To mark some parameters in your neural network as frozen parameters.\n",
        "*   To **speed up computations** when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "kIf8HXeRJcn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing Model Parameters"
      ],
      "metadata": {
        "id": "pNmsxveXJ1Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "ZbA23jYlJmUJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "4SP6YTjlJ-Tc"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lGMfcx_ZKG5g"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Sx1ef0FzKg1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "\n",
        "*   Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "*   Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "*   Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass.\n",
        "\n"
      ],
      "metadata": {
        "id": "x_pLGcRtKvUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "4RRGSBKHLIzG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "krh4WdwlLSkU",
        "outputId": "15de5517-17bc-413e-c1ea-0393543f00e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.294467  [   64/60000]\n",
            "loss: 2.279905  [ 6464/60000]\n",
            "loss: 2.262709  [12864/60000]\n",
            "loss: 2.258878  [19264/60000]\n",
            "loss: 2.232284  [25664/60000]\n",
            "loss: 2.207654  [32064/60000]\n",
            "loss: 2.212223  [38464/60000]\n",
            "loss: 2.170112  [44864/60000]\n",
            "loss: 2.171514  [51264/60000]\n",
            "loss: 2.139731  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 2.130175 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.148977  [   64/60000]\n",
            "loss: 2.130023  [ 6464/60000]\n",
            "loss: 2.067395  [12864/60000]\n",
            "loss: 2.083920  [19264/60000]\n",
            "loss: 2.021063  [25664/60000]\n",
            "loss: 1.966484  [32064/60000]\n",
            "loss: 1.992339  [38464/60000]\n",
            "loss: 1.899810  [44864/60000]\n",
            "loss: 1.912789  [51264/60000]\n",
            "loss: 1.844294  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.9%, Avg loss: 1.835316 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.881495  [   64/60000]\n",
            "loss: 1.840031  [ 6464/60000]\n",
            "loss: 1.713090  [12864/60000]\n",
            "loss: 1.760928  [19264/60000]\n",
            "loss: 1.647264  [25664/60000]\n",
            "loss: 1.611590  [32064/60000]\n",
            "loss: 1.634460  [38464/60000]\n",
            "loss: 1.528120  [44864/60000]\n",
            "loss: 1.559500  [51264/60000]\n",
            "loss: 1.467350  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.4%, Avg loss: 1.476882 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.551656  [   64/60000]\n",
            "loss: 1.511806  [ 6464/60000]\n",
            "loss: 1.356986  [12864/60000]\n",
            "loss: 1.438284  [19264/60000]\n",
            "loss: 1.321230  [25664/60000]\n",
            "loss: 1.326173  [32064/60000]\n",
            "loss: 1.342165  [38464/60000]\n",
            "loss: 1.259842  [44864/60000]\n",
            "loss: 1.294870  [51264/60000]\n",
            "loss: 1.213498  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.7%, Avg loss: 1.230421 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.311183  [   64/60000]\n",
            "loss: 1.288242  [ 6464/60000]\n",
            "loss: 1.120282  [12864/60000]\n",
            "loss: 1.231766  [19264/60000]\n",
            "loss: 1.111822  [25664/60000]\n",
            "loss: 1.140158  [32064/60000]\n",
            "loss: 1.162889  [38464/60000]\n",
            "loss: 1.093146  [44864/60000]\n",
            "loss: 1.129346  [51264/60000]\n",
            "loss: 1.063151  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 1.075404 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.149981  [   64/60000]\n",
            "loss: 1.146053  [ 6464/60000]\n",
            "loss: 0.961981  [12864/60000]\n",
            "loss: 1.099817  [19264/60000]\n",
            "loss: 0.979605  [25664/60000]\n",
            "loss: 1.012318  [32064/60000]\n",
            "loss: 1.048843  [38464/60000]\n",
            "loss: 0.985276  [44864/60000]\n",
            "loss: 1.019159  [51264/60000]\n",
            "loss: 0.966212  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.0%, Avg loss: 0.972901 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.035332  [   64/60000]\n",
            "loss: 1.052054  [ 6464/60000]\n",
            "loss: 0.851176  [12864/60000]\n",
            "loss: 1.010624  [19264/60000]\n",
            "loss: 0.894084  [25664/60000]\n",
            "loss: 0.920699  [32064/60000]\n",
            "loss: 0.971771  [38464/60000]\n",
            "loss: 0.913743  [44864/60000]\n",
            "loss: 0.941993  [51264/60000]\n",
            "loss: 0.900058  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.4%, Avg loss: 0.901749 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.949267  [   64/60000]\n",
            "loss: 0.985765  [ 6464/60000]\n",
            "loss: 0.770566  [12864/60000]\n",
            "loss: 0.946976  [19264/60000]\n",
            "loss: 0.836318  [25664/60000]\n",
            "loss: 0.853395  [32064/60000]\n",
            "loss: 0.916335  [38464/60000]\n",
            "loss: 0.865129  [44864/60000]\n",
            "loss: 0.886495  [51264/60000]\n",
            "loss: 0.851961  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 0.850196 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.882495  [   64/60000]\n",
            "loss: 0.935670  [ 6464/60000]\n",
            "loss: 0.709997  [12864/60000]\n",
            "loss: 0.899703  [19264/60000]\n",
            "loss: 0.794609  [25664/60000]\n",
            "loss: 0.802372  [32064/60000]\n",
            "loss: 0.874036  [38464/60000]\n",
            "loss: 0.830938  [44864/60000]\n",
            "loss: 0.845018  [51264/60000]\n",
            "loss: 0.814958  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.0%, Avg loss: 0.811022 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.828719  [   64/60000]\n",
            "loss: 0.895508  [ 6464/60000]\n",
            "loss: 0.662784  [12864/60000]\n",
            "loss: 0.863554  [19264/60000]\n",
            "loss: 0.762788  [25664/60000]\n",
            "loss: 0.763106  [32064/60000]\n",
            "loss: 0.839746  [38464/60000]\n",
            "loss: 0.805474  [44864/60000]\n",
            "loss: 0.812895  [51264/60000]\n",
            "loss: 0.785374  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.779847 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Load the Model"
      ],
      "metadata": {
        "id": "MsTsrHAfMNTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "qgujoM0aMP0-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "id": "vezzNpe_MXbJ",
        "outputId": "1c8e6a5f-1766-42bb-acfb-7dc47de92e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:08<00:00, 66.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code below, we set `weights_only=True` to limit the functions executed during unpickling to only those necessary for loading weights. Using `weights_only=True` is considered a best practice when loading weights."
      ],
      "metadata": {
        "id": "wp5QSh4kMqw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n",
        "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "f5ddB0qSMYSH",
        "outputId": "e21f9357-3af1-4539-a1b3-b80775aa4d0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass `model` (and not `model.state_dict()`) to the saving function:"
      ],
      "metadata": {
        "id": "KxvFAXftNDK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "tIlsqqfuM5GI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As described in Saving and loading torch.nn.Modules, saving `state_dict` is considered the best practice. However, below we use `weights_only=False` because this involves loading the model, which is a legacy use case for `torch.save`.\n",
        "\n"
      ],
      "metadata": {
        "id": "i8nl0sn7NVcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pth', weights_only=False),"
      ],
      "metadata": {
        "id": "-TcE7kNPNL2W"
      },
      "execution_count": 53,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}